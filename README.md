# FarmIA – Kafka and Real-Time Processing

This project aims to build a real-time processing pipeline for FarmIA using Apache Kafka and its ecosystem. The system enables integration of agricultural sensor data (IoT) and online sales transactions, generating valuable insights for decision-making.

## Requirements

You must have **Terraform installed** on your system to deploy the infrastructure.  
You can download it from: [https://www.terraform.io/downloads.html](https://www.terraform.io/downloads.html)

### Provider Versions

This project uses the following provider versions:

```hcl
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "=3.0.0"
    }
    confluent = {
      source  = "confluentinc/confluent"
      version = "=2.34.0"
    }
  }
}
```

## Project Objectives

Build a Kafka-based streaming solution that:

1. Processes sensor data in real-time to detect anomalous conditions.
2. Processes transaction data in real-time to generate minute-by-minute aggregated summaries.
3. Uses KSQLDB to generate:
   - Alerts for sensor anomalies.
   - Sales summaries by product category every minute.

---

## Repository Structure

```plaintext
├── LICENSE
├── README.md                    # This file
└── terraform/
    ├── logs/                    # Execution logs for KSQL queries
    │   ├── ksql-sales_data.log
    │   ├── ksql-sales_summary.log
    │   ├── ksql-sensor_alerts.log
    │   └── ksql-sensor_data.log
    ├── main.tf                  # Main Terraform file
    ├── terraform.tfvars.example # Configuration template
    ├── variables.tf             # Global variable definitions
    ├── modules/
    │   ├── assets/
    │   │   ├── avro_schemas/    # AVRO schemas (sensor and transactions)
    │   │   ├── ksql/            # KSQL queries in JSON format
    │   │   └── scripts/         # Auxiliary scripts like startup.sh
    │   ├── azure_core/          # Azure infrastructure for MySQL database
    │   ├── confluent_cluster/   # Kafka cluster configuration
    │   ├── confluent_env/       # Confluent environment variables
    │   ├── connectors/          # Kafka Connect connector definitions
    │   ├── kafka_topics/        # Kafka topic declarations
    │   ├── ksqldb/              # KSQLDB configuration
    │   ├── schema_registry/     # Schema registry
    │   └── schemas/             # Schema management via Terraform
````

## Environment Setup

1. Copy and edit the necessary variables:

   ```bash
   cp terraform.tfvars.example terraform.tfvars
   ```

2. Adjust the variables in `terraform.tfvars` according to your environment.

3. Apply the infrastructure:
   ```bash
   cd terraform
   terraform apply -auto-approve
   ```

---

## AVRO Schemas

The schemas used for data serialization in Kafka are defined in AVRO format and can be found in the [`terraform/modules/assets/avro_schemas/`](https://github.com/orr21/FarmIA-Kafka/blob/main/terraform/modules/assets/avro_schemas/) directory.

---

## Kafka Topics

The following topics are automatically created and used during the processing flow:

- `_transacions`: simulates incoming transactions that are stored in MySQL.
- `sensor-telemetry`: receives data generated by IoT sensors.
- `sales-transactions`: receives sales data extracted from MySQL.
- `sensor-alerts`: receives alerts generated by sensor anomalies.
- `sales-summary`: contains sales summaries by category.

The topic configuration can be found in the [`terraform/main.tf`](https://github.com/orr21/FarmIA-Kafka/blob/main/terraform/main.tf#L116) file, around line 116.

---

## Kafka Connect

Connectors are used to integrate external sources with Kafka:

- **Datagen Connector**:
  - Simulates agricultural sensor data in the `sensor-telemetry` topic.
  - Simulates transaction data in the `_transacctions` topic.

- **MySQL Sink Connector**: 
  Connects to the relational database and inserts transactions extracted from `_transactions`.

- **MySQL Source Connector**:
  Connects to the relational database and publishes to `sales-transactions`.

The connector configuration can be found in the [`terraform/main.tf`](https://github.com/orr21/FarmIA-Kafka/blob/main/terraform/main.tf#L140) file, around line 140.

---

## KSQL

Real-time processing is performed using KSQLDB. Two main flows are implemented:

1. **Anomaly Detection (Sensors)**

   - Input: `sensor-telemetry`
   - Conditions: temperature > 35 °C or humidity < 20 %
   - Output: `sensor-alerts`

2. **Sales Summary by Category**
   - Input: `sales-transactions`
   - Aggregates total revenue by product category every minute
   - Output: `sales-summary`

The queries can be found in [`modules/assets/ksql/`](https://github.com/orr21/FarmIA-Kafka/blob/main/terraform/modules/assets/ksql/).

---

## Technical Challenges

### KSQL Integration with Terraform

One of the most significant challenges in this project was the limited compatibility between the Confluent Terraform provider and KSQLDB. At the time of development, the Confluent provider (version 2.34.0) did not offer native resources for creating and managing KSQL streams and tables declaratively.

**The Solution:**

To overcome this limitation, a custom approach was developed using Terraform's `null_resource` with `local-exec` provisioners. This solution:

1. Creates the KSQLDB cluster using the native `confluent_ksql_cluster` resource
2. Generates API credentials for authentication
3. Executes KSQL statements by making direct HTTP requests to the Confluent KSQL REST API using `curl`
4. Stores execution logs for debugging and auditing purposes

The implementation can be found in [`terraform/modules/ksqldb/main.tf`](https://github.com/orr21/FarmIA-Kafka/blob/main/terraform/modules/ksqldb/main.tf), where each KSQL query is executed as a separate `null_resource` with proper dependency management to ensure correct execution order.

This approach, while not ideal from a pure Infrastructure-as-Code perspective, provides a reliable and reproducible way to deploy KSQL resources alongside the rest of the Kafka infrastructure.

---

## Environment Shutdown

For security and explicit control reasons, infrastructure shutdown must be done manually. This prevents accidental deletion of critical resources in shared or production environments.

If you try to run:

   ```bash
   terraform destroy -auto-approve
   ```

You will see a permission denied error. This restriction is intentional. If you need to delete resources, contact the environment administrator or follow the authorized procedure within your cloud environment.
